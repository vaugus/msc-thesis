Either in microscopic or macroscopic scales, imaging devices have a finite depth of field. Blurry regions in images may occur due to several reasons, e.g. the acquisition outside the boundaries of the depth of field \cite{huang2007evaluation}. Therefore, it is nearly impossible to obtain a globally sharp image in microscopic scales, since height differences within the surface of the sample will also be magnified and the depth of field of microscopes is usually small. Image fusion of stack of images, acquired with a variety of focus adjustments, may yield a fully sharp image. In this section, we investigate multifocus image fusion techniques based on principal component analysis, guided filtering with local linear models, gradients and singular value decomposition. Similarly to the proposed IQA method, our image fusion approach explores one of the well-known literature methods, i.e. a gradient-based analysis to distinguish sharp and blurry regions.

\subsection{Pixel-level image fusion using wavelets and principal component analysis}

\citeonline{naidu2008pixel} addressed the multifocus image fusion problem with two fusion rules, based on \sigla{PCA}{Principal Component Analysis} and the \sigla{DWT}{Discrete Wavelet Transform}. The Principal Component Analysis is a mathematical procedure based on linear algebra, which performs the extraction of uncorrelated variables that represent data. In other words, the PCA based fusion rule composes the image from the most relevant subset of data from each input image. The proposed PCA-based image fusion algorithm consists of the following steps:

\begin{enumerate}[label=\Roman* -]
    \item First and foremost, reshape the input images as two-column vectors, which yields a matrix of dimensions $2 \times n$, with $n$ as the original dimension of each image. Let the resulting matrix be denoted as $Z$;
    
    \item Compute the empirical mean vector of dimensions $1 \times 2$, i.e. a vector that contains the mean of observations for each variable;
    
    \item Subtract the empirical mean vector from each column of $Z$, which results in a matrix of dimensions $2 \times n$. Let the resulting matrix be denoted as $Y$;
    
    \item Compute the covariance matrix of $Y$, i.e. $YY^{T}$;
    
    \item Compute the eigenvectors and the eigenvalues of the covariance matrix and sort them by descending order. Let the eigenvectors and the eigenvalues be denoted by $V$ and $D$, respectively. Both $V$ and $D$ dimensions are $2 \times 2$;
    
    \item Compute the principal components $P_{1}$ and $P_{2}$ with the first column of $V$, as
    
    \begin{align}
    P_{1} = \frac{V(1)}{\sum V}
    &&
    P_{2} = \frac{V(2)}{\sum V}.
    \end{align}
\end{enumerate}

\noindent Finally, the fused image is composed by

\begin{equation}
g_{fused}(x,y) = P_{1}g_{1}(x,y) + P_{2}g_{2}(x,y),
\end{equation}

\noindent where $g_{fused}$ represents the fused image and $g_{1}$ and $g_{2}$ represent the input images, respectively. The process may be extended to several images by applying the method in the first two images of the dataset, then with the fused image and another image and so on.

Along with PCA, the authors explored a fusion approach with the DWT. The transform is applied to the input images and decomposes them into subbands of detailed and approximation wavelet coefficients, i.e. high and low-frequency components, respectively. Finally, the fusion rule consists of averaging the approximation coefficients and choosing the maximum among each detailed coefficient in each subband.

\subsection{Image fusion with guided filtering}

The guided filtering, i.e. a windowed linear transform of the input image where coefficients are obtained by linear ridge regression was applied to perform image fusion \cite{li2013image}. The authors proposed a pixel-level image fusion technique that also makes use of low pass filtering, edge extraction, and spatial consistency. It consists of three steps: two-scale image decomposition, weight map construction with guided filtering and two-scale image reconstruction.

The decomposition step transforms the input images into two-scale representations by means of a convolution with an average filter of size $31 \times 31$. The first representation is named \emph{base} layer, directly obtained from the average filter convolution, and the second representation is named \emph{detail} layer, obtained by subtracting the base layer from the input image.

In order to retrieve the weight map, i.e. characterization of sharp and blurry regions of the image, edge extraction is performed by means of Laplacian filtering with a $3 \times 3$ kernel (which will be further explained in \autoref{chapter:materials-and-methods}); subsequently, the edges undergo a Gaussian low-pass filter of dimensions $11 \times 11$ and $\sigma = 5$. Let $S_{i}^{k}$ be the smoothed edge in the pixel $k$ of the $i$th input image. The weight map is built as

\begin{equation}
\label{eqn:weight_map}
P_{i}^{k} = 
    \begin{cases}
        1, & \text{if } S_{i}^{k} = \max{(S_{1}^{k},S_{2}^{k},\dots,S_{n}^{k})}\\
        0, & \text{otherwise}
    \end{cases},
\end{equation}

\noindent where $n$ is the number of input images. The guided filter is then applied to each image, together with its weight map, which produces final weight matrices $W_{i}^{B}$ and $W_{i}^{D}$ for the base and detail layers of each image, respectively. Subsequently, all base and detail layers are fused by means of weighted averaging with their respective weight matrices, given by

\begin{align}
\bar{B} = \sum_{i=1}^{n}W_{i}^{B}B_{i}
&&
\bar{D} = \sum_{i=1}^{n}W_{i}^{D}D_{i},
\end{align}

\noindent where $\bar{B}$ and $\bar{D}$ are the fused base and detail layers, respectively. The fused image is finally constructed by summing the fused layers.

\subsection{Multi-scale weighted gradient-based fusion for multi-focus images}

Another way to perform edge-detection based image fusion is to extract gradients from each image among a set and merge them with the structure tensor, as proposed by \citeonline{zhou2014multi}. The first processing step in this approach is to locally obtain the gradients of each input image in the form of a structure tensor, i.e. a non-negative second-moment matrix where its eigenvalues represent the intensity changes in a given point, and then a local gradient covariance matrix described as

\begin{equation}
C_{\sigma} =
    \begin{pmatrix}
        \nabla_{x}^{2} \ast G_{\sigma}
        &
        \left(
            \nabla_{x}\nabla_{y} \ast G_{\sigma}
        \right)
        \\
        \left(
            \nabla_{x}\nabla_{y} \ast G_{\sigma}
        \right)
        &
        \nabla_{x}^{2} \ast G_{\sigma}
    \end{pmatrix},
\end{equation}

\noindent where $G_{\sigma}$ is a Gaussian filter, $\nabla_{x}$ and $\nabla_{y}$ are the gradients of the image along $x$ and $y$ directions in a given pixel. The $\sigma$ parameter represents the standard deviation of the Gaussian function and also the scale of the matrix $C_{\sigma}$. The local image structure is related to the eigenvalues of $C_{\sigma}$, $s_{1}^{2}$ and $s_{2}^{2}$. Let $s_{1}$ and $s_{2}$ be the square root of such eigenvalues. If one of those is large and the other is small, it means that the pixel consists of an edge; if both are large, then the region is sharp, which indicates a corner. Then, the authors define a structure saliency measure for the image at a selected scale, as

\begin{equation}
\label{eqn:structure_saliency}
Q = \sqrt{(s_{1} + s_{2})^{2} + \alpha(s_{1} - s_{2})^{2}},
\end{equation}

\noindent where $\alpha > -1$. This is used as a sharpness measure for the multifocus images at different scales in the presence of anisotropic blur and also misregistration. The sharpness is first computed at a large scale, i.e. a large $\sigma$ value, which roughly detects the focused region of each input image.

With a coarse representation of the focused regions in hands, the next step is to define an unknown region near the boundaries of each focused region and set the gradient weights as $1$ for sharp and $0$ for blurry. The gradient weights are then calculated by applying the focus measure with a small $\sigma$ value and then merged with the gradients. This results in a set of probabilities for each pixel in each image. The composition of the fused image is then guided by the largest probability values.

\subsection{Image fusion technique using multi-resolution singular value decomposition}

\citeonline{naidu2011image} proposed a fusion technique which is similar to the DWT-based ones, but employs a Multiresolution \sigla{SVD}{Singular Value Decomposition} instead. The multiresolution SVD performs a factorization of a matrix into eigenvalues and eigenvectors in different scales by the following steps:

\begin{enumerate}[label=\Roman* -]
    \item Let $X = [x_{1},x_{2},\dots,x_{n}]$ be an one-dimensional signal of length $n$. Reshape $X$ onto a matrix given by 
    
    \begin{equation*}
        X_{1} = \begin{bmatrix}
                x_{1} & x_{3} & \dots & x_{n-1} \\
                x_{2} & x_{4} & \dots & x_{n}
                \end{bmatrix}
    \end{equation*}
    
    and a scatter matrix $T_{1} = X_{1}X_{1}^{T}$;
    
    \item Factorize the matrix $T_{1}$. Let the eigenvector matrix of $T_{1}$ be $U_{1}$. Then $T_{1}$ is transformed into a diagonal matrix $S_{1}^{2}$ as
    
    \begin{equation*}
        S_{1}^{2} = U_{1}^{T}T_{1}U_{1} =
        \begin{bmatrix}
            s_{1}^{2} & 0\\
            0 & s_{2}^{2}
        \end{bmatrix},
    \end{equation*}
    
    where $s_{1}^{2}$ and $s_{2}^{2}$ are the squares of the singular values. Then, let $\hat{X}_{1} = U_{1}^{T}X_{1}$. The top row of $\hat{X}_{1}$ contains the approximation component, i.e. the largest eigenvalue, and the bottom row contains the detail component, i.e. the smallest eigenvalue. Let $\varphi$ and $\psi$ represent the top and bottom rows of $\hat{X}_{1}$, respectively;
    
    \item In order to perform the image decomposition in other scales, the approximation component $\varphi$ plays the role of $X$.

\end{enumerate}

The MSVD is a decomposition of $2 \times 2$ blocks of the image, rearranged into one-dimensional vectors of dimensions $4 \times 1$. The authors proposed a two-level decomposition, which generates 8 images from each input image. At each level, the largest absolute value among the MSVD detailed coefficients will compose the fused image, while at the coarsest level, the average of all approximation is computed and added to the output.