
{\color{blue} TRANSFORMS AND FUSION SHOULD BE MOVED TO IMAGE PROCESSING CHAPTER}

% \section{Image Transforms}

% Usually, the most trivial image processing operations are done at the pixel level; they act on one of the channels of an image from an specific color space, on gray levels of a grayscale image and so on. Some procedures such as the convolution have the computational performance as a bottleneck, and this is one of the reasons why \emph{image transforms} are widely used. They encompass any group of mathematical operations that takes the input signal or the input image out of their domain an project them onto the transformed domain \cite{gonzalez2008digital}. The convolution operation, for instance, turns itself into a simple matrix multiplication task on the Fourier Transform domain (which will be detailed in the following sections) and that solves the performance bottleneck. The general structure of a forward image transform and an inverse image transform is denoted by  the pair of equations \ref{eqn:generic_transform}, respectively:


% \begin{align}
%     \label{eqn:generic_transform}
%     T(u,v) = 
%     \sum_{x=0}^{M-1}
%     \sum_{y=0}^{N-1}f(x,y)r(x,y,u,v)
%     &&
%     f(x,y) = 
%     \sum_{x=0}^{M-1}
%     \sum_{y=0}^{N-1}T(u,v)s(x,y,u,v)
% \end{align}

% \noindent where $M$ and $N$ are the dimensions of the image, $x$ and $y$ are coordinates of the image, $u = \{0,1,2,...,M-1\}$ and $v = \{0,1,2,...,N-1\}$ are called transform variables, $r(x,y,u,v)$ is a function named \emph{forward transform kernel} that is responsible for the forward domain change and finally $s(x,y,u,v)$ is the inverse kernel for $r$. These equations take an image to another domain, perform some operations and then return to the spatial domain. These are the common steps in transforms applications indeed. The following sections contain details about relevant image transforms in image segmentation and image fusion.

% \subsection{Fourier Transform}

% The Fourier Transform is a mathematical framework that expresses a periodic or non-periodic function with a sum of weighted sinusoids, what results in a set of different frequencies \cite{gonzalez2008digital}. The transform relies on complex numbers and the Euler's formula. The complex numbers belong to the broadest of the number sets, i.e. $\mathbb{R} \subseteq \mathbb{C}$, and consists of a \emph{real part} $a$ and an \emph{imaginary part} $bj$, shown by equation \ref{eqn:complex_number}:

% \begin{equation}
% \label{eqn:complex_number}
%     z = a + bj
% \end{equation}

% \noindent where $j = \sqrt{-1}$ is the imaginary unit. In this sense, the Euler's formula relates complex exponential functions with trigonometric functions through the equation  \ref{eqn:euler_formula}:

% \begin{equation}
% \label{eqn:euler_formula}
%     e^{j\theta} = \cos{\theta} + j\sin{\theta}
% \end{equation}

% \noindent where $\theta \in \mathbb{R}$ is a number that represents an angle in radians. This is the fundamental framework to describe the Fourier transform, since it is a sum of function representations by means of the Euler's formula. According to \citeonline{brigham1988fast}, the Continuous and the \sigla{DFT}{Discrete Fourier Transform} in a simplified version can be computed as shown on the left and on the right of Equation \ref{eqn:fourier_transform}, respectively:

% \begin{align}
% \label{eqn:fourier_transform}
%     H(x) = \int_{-\infty}^{\infty}h(t)e^{-j2 \pi tx}dt
%     &&
%     H(x) = \sum_{x=0}^{N-1}h(t)e^{-j2 \pi t \frac{x}{N}}
% \end{align}

% \noindent with $H(x)$ as the Fourier transform of a function $h(t)$, $t$ represents functions is the time domain, $x$ represents the frequency domain and $N$ represents the total number of samples on the discrete approach. As already pointed out, images can be interpreted as two-dimensional functions; hence, the most important version of the Fourier transform in the context of image processing is the two-dimensional DFT, given by Equation \ref{eqn:2D_dft}:

% \begin{equation}
% \label{eqn:2D_dft}
%     H(u,v) = \frac{1}{\sqrt{MN}}\sum_{x=0}^{M-1}\sum_{y=0}^{N-1}h(x,y)e^{-j2 \pi \big(\frac{ux}{M} + \frac{vy}{N}\big)}
% \end{equation}

% \noindent where $h(x,y)$ consists of an image, $M$ and $N$ are the image dimensions and $u$ and $v$ are coordinates in the frequency domain. This operation is extremely useful for almost every field related to analysis of functions. However, the DFT consists of a convolution process with $O(n^{2})$ asymptotic complexity, which can be too slow for larger dimensions. The \sigla{FFT}{Fast Fourier Transform} is an algorithm to compute the DFT by taking small slices of the signal, computing the transform with them and them merging the result. This procedure is performed in $O(n \log n)$ complexity, which is reasonable for many applications.


% \section{Image Fusion}

% Immediately upon the blur map extraction is satisfactorily complete, an image fusion operation is needed in order to achieve the highest amount of focused regions as possible. Image fusion is a process that merges several images (possibly taken in diverse conditions or with different cameras) into one image with higher quality, more details and consequently more useful for humans and computer tasks \cite{mitchell2010image}. Image fusion techniques can be used in noise reduction, edge enhancement and super-resolution, for example. 

% One prominent use of image fusion occurs in medical image fields; the quality of information about illnesses, cells, clinical analysis and several other medical tasks (including the computer assisted ones) have found profitable results from the techniques and lead themselves to better and faster decisions when it comes to human beings \cite{james2014medical}.

% There are relevant applications in remote sensing multispectral images, segmentation of regions in different colourspaces, biometry: the pan-sharpening process is the generation of an high resolution multispectral image from low to high resolution ones, K-Means segmentation and fusion of pixels in the \sigla{RGB}{Red, Green and Blue colourspace} and the Iris Recognition biometric process with video frames are examples of such tasks, respectively \cite{mitchell2010image}.

% \subsection{General Framework}

% Still according to \citeonline{mitchell2010image}, the general framework for the image fusion procedure can be accomplished in four stages: Multiple Input Images, Common Representational Format, Fusion and Display.
% The \emph{multiple input images} stage consists in obtaining the set of images which will be merged. There are several approaches to this: the dataset may be captured from different sensors, on distinct light conditions or angles, with different magnifications, on several focus and with temporal measurements if the scene changes through time.

% After the image set generation, there is the necessity of reshaping each item. This configures the \emph{common representational format}, responsible for creating a new and temporary dataset with the same properties, e.g. colourspace, dimensions, noise level and so on. The \emph{fusion} stage consists of using a decision method in order to dictate which regions, objects, colours or details will compose the final image; this can be done by any approach that relates to how the result should be. There are some methods that rely on the Wavelet Transform domain, for example. Finally, the \emph{display} stage consists in providing a view for the resulting image, which can be used directly for any further task or even be the input for other image processing operation. Figure \ref{fig:fusion_general_framework} depicts an arbitrary example of the four stages:

% \begin{figure}[H]
% 	\centering
% 	\caption{\label{fig:fusion_general_framework}Image fusion general framework. (a) Multiple Input Images, (b) Common Representational Format, (c) Fusion and (d) Display.}
% 	\begin{center}
% a 	    \includegraphics[scale=0.4]{images/fig10.png}
% 	\end{center}
% 	\centering
%     \fautor
% \end{figure}

% The four arbitrary images in Figure \ref{fig:fusion_general_framework}.\textbf{(a)} represents different images of the same scene, taken at different resolutions, rotations and shapes. In \ref{fig:fusion_general_framework}.\textbf{(b)}, the images are all reshaped, converted to the same colourspace and ready to receive processing algorithm which will transform them into feature vectors. Figure \ref{fig:fusion_general_framework}.\textbf{(c)} represents the image fusion by means of an arbitrary method. The resulting image is depicted in Figure \ref{fig:fusion_general_framework}.\textbf{(d)}.

% Since image fusion is only one branch of data fusion field, this procedure has a wide variety approaches and methods; hence, the domain will be restricted to the multifocus image fusion and some related work will be presented as follows, in section \ref{sec:multifocus_image_fusion}.
